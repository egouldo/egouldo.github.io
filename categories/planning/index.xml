<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Planning on Elise Gould: PhD Research Notebook</title>
    <link>/categories/planning/</link>
    <description>Recent content in Planning on Elise Gould: PhD Research Notebook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>elise.gould@unimelb.edu.au (Elise Gould)</managingEditor>
    <webMaster>elise.gould@unimelb.edu.au (Elise Gould)</webMaster>
    <copyright>(c) 2018 -- All rights reserved.</copyright>
    <lastBuildDate>Fri, 21 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/planning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Meeting-20-Dec-2018</title>
      <link>/posts/meeting-20-dec-2018/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/meeting-20-dec-2018/</guid>
      <description>Present: HF, FF, LR, EG
Problems with the proposed QRP modelling method HF How necessary is it going to be to have covariates at the study/trial level? It will poptentially be cumbersome and problematic. Can we just have an uninformative error term to soak up between-study variation?
cumbersome to implement
 must pre-identify articles for each participant, asking them about specific papers (logistically problematic, how do we find appropriate studies? The author must also be the person who conducted the analyses, and it may not be easy to find contact details for them) ask participants to specify a fixed number of studies in which they used the analysis type (although not necessary for the model, restricting n is going to be better for the people answering the question, and more tractible for me)… then we need to pipe these studies through to the rest of the survey some how.</description>
    </item>
    
    <item>
      <title>Replicating Models - Talks with Neil</title>
      <link>/posts/replicating-models-talks-with-neil/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/replicating-models-talks-with-neil/</guid>
      <description>Experiment Description ft. God, Mere mortals and steve the synthesiser. UPDATE WITH NOTES FROM NEIL MEETING
 Reasons why this is a good idea “GOD” – We never have truth to compare to! This will be a unique opportunity to be able to properly evaluate models and their deviation from some truth. We can see where in the causal strucutre people keep getting things wrong – are there commonalities? Is there overlap in parts of their model structure?</description>
    </item>
    
    <item>
      <title>Meeting-Dec-2018-Jian</title>
      <link>/posts/meeting-dec-2018/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/meeting-dec-2018/</guid>
      <description>Really good meeting with Jian, lots of ideas.
Things to look up:
 Near-term forecasting (as a reproducibility / RDF ) Multiverse problem ~ are there parallels to the simulation study Neil and I have been talking about?  Multiverse problem intra versus inter person variation.
Can you replicate a multiverse for an individual? Get people to come up with 10 models each and then compare.
So there’s model uncertainty and then subjective uncertainty (Ray and Burgman).</description>
    </item>
    
    <item>
      <title>confirmationnotes</title>
      <link>/posts/confirmation-notes/</link>
      <pubDate>Mon, 10 Dec 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/confirmation-notes/</guid>
      <description>Here are my notes of the issues raised during the confirmation meeting to be followed up, and some initial thoughts / responses to them.
1. Scope: reproducibility of decisions vs. reproducibility for decisions. PV took issue with the framing / scope of the PhD – namely that tackling reproducibility of decisions is too big of a task for the PhD. Need to first evaluate reproducibility for decisions first.
I agree with PV’s point that looking at reproducibility of decisions is too big of a task for this PhD.</description>
    </item>
    
    <item>
      <title>Ropensci</title>
      <link>/posts/ropensci/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/ropensci/</guid>
      <description>Problem Replication effort:
There are many sources of error (as to why claim is not supported, or approximate result[significance in same direction, effect size]):
 actual data is not reflected in code files Can’t match the environment: Missing packages etc, etc, etc  One model of addressing these issues is using Docker:
Docker goes some way to solving this…. BUT: It’s really hard to get docker up and running - you need specialised knowledge of how to get your code and data etc.</description>
    </item>
    
    <item>
      <title>updated-literature-workflow</title>
      <link>/posts/updated-literature-workflow/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/updated-literature-workflow/</guid>
      <description>Capture papers annotations
Highlights workflow Colours for each different type of annotation References to follow up Metadata
Summarise .md and exporting to DEVONthink.
  Organise The two main apps used are Bookends and DEVONthink. The key difference between the material and level of organisation that goes into these two apps is as follows:
 DEVONthink: project / paper specific organisation. Bookends: general organisation, relevant to any writing task.  Even if a subject area keyword is consistent across both Bookends and DEVONthink, DEVONthink will contain only a subset of those in Bookends - i.</description>
    </item>
    
    <item>
      <title>Meeting 20 September 2018</title>
      <link>/posts/meeting-2018-sep-20/</link>
      <pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/meeting-2018-sep-20/</guid>
      <description>Present: HF, LR, EG.
Divying up the Thesis Stepping back after a couple of talks and much RA-ing… Today we discussed the division of the QRP research material I have been working on in terms of research outputs / papers. We then discussed the overall plan for the thesis, and reflected back on the original plan discussed at the pre-confirmation meeting.
The QRP material is to be divided into two papers:</description>
    </item>
    
    <item>
      <title>Thesis Bootcamp</title>
      <link>/posts/thesis-bootcamp/</link>
      <pubDate>Wed, 19 Sep 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/thesis-bootcamp/</guid>
      <description>Date of the Bootcamp: 19th - 21st October https://gradresearch.unimelb.edu.au/preparing-my-thesis/writing-the-thesis https://gradresearch.unimelb.edu.au/preparing-my-thesis/thesis-with-publication
Please provide a brief description of your research area/interests as relevant to your thesis: 
PhD Title: Transparency and Reproducibility of Decision Support Tools in Ecology and Conservation. This includes fields, such as: meta-research / meta-science, open science, conservation decision-making, structured decision making and decision theory / decision analysis, as well as ecological modelling.
Why would you like to participate in the Thesis Boot Camp?</description>
    </item>
    
    <item>
      <title>Meeting 23 August</title>
      <link>/posts/meeting-23-august/</link>
      <pubDate>Thu, 23 Aug 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/meeting-23-august/</guid>
      <description>Tasks from last week: Develop QRP list across all methods. Figure out what is general between each of the methods. Then come up with a list of QRPs specific to the method under consideration. Also think about the overarching decision / analytic process for each of the methods. Could the QRP occur at multiple points along the workflow / model building pipeline? Sampling protocol - begin writing code, journal selection. Think about elicitation measures, what they could be, and how they could be the same across each of the methods  Other things to discuss today:</description>
    </item>
    
    <item>
      <title>QRPs Study Planning</title>
      <link>/posts/qrps-study-planning/</link>
      <pubDate>Thu, 14 Jun 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/qrps-study-planning/</guid>
      <description>QRPs for non-hypothesis testing research in Ecology and Conservation Decision Making Problem and Background:
The reproducibility literature has focused exclusively on hypothesis-testing, whether that be Bayesian or frequentist. This also applies to initial research focusing on ecology and evolution. However, Fidler (2016) correctly identifies that in applied ecological research, particularly in conservation science, non-hypothesis testing methods, such as decision-theory, cost-effectiveness analysis, optimization and other scientific computing methods are common. These approaches come with their own set of reproducibility issues.</description>
    </item>
    
    <item>
      <title>Advisory Committee Meeting</title>
      <link>/posts/advisory-committee-meeting/</link>
      <pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/advisory-committee-meeting/</guid>
      <description>Chapter 2, QRPs Scope Trying to cover the scope of the entire decision process might be too large for just one chapter. Just examining QRPs for system modelling is quite a task. PV suggested restricting the focus to decision tools and focussing on one application area, such as conservation planning / reserve design. This would also give more traction and uptake among ecologists. Conservation planning isn&amp;rsquo;t using the term &amp;lsquo;reproducibility&amp;rsquo; to describe their problems.</description>
    </item>
    
    <item>
      <title>PhD Research Proposal: Reproducibility and Transparency of Decisions in Ecology and Conservation</title>
      <link>/posts/research_proposal/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/research_proposal/</guid>
      <description>Keywords:
 conservation decision-making ecology reproducibility structured decision-making  Introduction Successful biodiversity conservation and management is underpinned by effective and robust decision-making (Mukherjee et al. 2018). Decision-makers are tasked with allocating limited resources in the face of uncertainty about the effectiveness of alternative management interventions, and incomplete or inadequate scientific information. Moreover, environmental decisions often must be made in complex socio-economic and political contexts, with multiple stakeholders and multiple and/or competing objectives.</description>
    </item>
    
    <item>
      <title>PhD Timeline and Milestones</title>
      <link>/posts/phd_timeline/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/phd_timeline/</guid>
      <description>Timeline &amp;lt;iframe src=“https://egouldo.shinyapps.io/shiny_phd_timeline/” style = &amp;quot;border: none; width: 900px; height: 1000px&amp;gt;
 Milestones Milestones are documented in the RHD manual, page 4: https://app.lms.unimelb.edu.au/bbcswebdav/pid-6166800-dt-content-rid-24849943_3/orgs/COM_01654/RHD%20Handbook%20v%202017%2008%2016%20LMS%20version%202.pdf
Candidature and Confirmation: Probationary Candidate: You start your PhD as a probationary candidate and within 12 months (we recommend the ideal time of 9 to 10 months) you will go through the confirmation procedure, formalising your candidature. Purpose of Confirmation: The main purpose of the confirmation process is to determine if you have developed a detailed research plan together with your supervisor(s), to assess whether this research plan meets the requirements of the degree, and to assess if adequate progress has been made.</description>
    </item>
    
    <item>
      <title>reproducibility criteria</title>
      <link>/posts/reproducibility-criteria/</link>
      <pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/reproducibility-criteria/</guid>
      <description>I need to start thinking about how we define reproducibility for a) ecological models b) ecological models in the context of decision support.
Reviewing Fiona&amp;rsquo;s article on metaresearch on ecology would be a good place to start thinking about reproducibility in ecology.
-&amp;gt; Reproducibility in ecology&amp;hellip; why important? &amp;ldquo;the inherent complexity of the inference chain needed to advance ecology as well as the importance of ecological results to challenges important to society&amp;rdquo; https://eco.</description>
    </item>
    
    <item>
      <title>first chapter</title>
      <link>/posts/first-chapter/</link>
      <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/first-chapter/</guid>
      <description>Libby has suggested an idea for the first chapter. It would be a data-based review of reproducibility issues facing decision support tools and ecological models. It would require not simply applying existing criteria for reproducibility, but would require generating criteria appropriate to DST&amp;rsquo;s and ecological models. Maybe some existing criteria apply. Maybe some don&amp;rsquo;t.
First chapter initial steps:
 identify why reproducibility is important for ecological models / decision support tools Generate a list of reproducibility criteria: - review issues facing devt of ecological models - review literature on reproducibility in ecology / conservation in general Generate a set of DST&amp;rsquo;s / models to apply the list to Figure out a way of scoring the models Apply the list and score the models  Questions:</description>
    </item>
    
    <item>
      <title>first post</title>
      <link>/posts/first-post/</link>
      <pubDate>Wed, 13 Dec 2017 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/first-post/</guid>
      <description>Readings: Reproducibility – how to define for this context. Ecology / decision support tools.
Reproducibility of ecological models used for decision support tools.
Interested in the sorts of decisions that we make as modellers… e.g. what performance measures do we use, there are so many different ways of valuing the objectives. And depending on which measure you use, you might judge the outcomes of each of the different strategies under consideration differently….</description>
    </item>
    
  </channel>
</rss>