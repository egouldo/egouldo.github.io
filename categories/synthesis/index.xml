<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Synthesis on Elise Gould: PhD Research Notebook</title>
    <link>/categories/synthesis/</link>
    <description>Recent content in Synthesis on Elise Gould: PhD Research Notebook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>elise.gould@unimelb.edu.au (Elise Gould)</managingEditor>
    <webMaster>elise.gould@unimelb.edu.au (Elise Gould)</webMaster>
    <copyright>(c) 2018 -- All rights reserved.</copyright>
    <lastBuildDate>Mon, 03 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/synthesis/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Choose your own adventure: researcher degrees of freedom and questionable research practices in ecological modelling for decision support</title>
      <link>/posts/modelling-workflows-researcher-degrees-of-freedom-and-transparency/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/modelling-workflows-researcher-degrees-of-freedom-and-transparency/</guid>
      <description>Abstract:
Initial meta-science research in ecology and evolution suggests that the discipline is not immune to the sorts of reproducibility issues highlighted in other scientific disciplines, such as Psychology and Medicine. A recent study has revealed rates of self-reported Questionable Reserach Practices (QRPs) in ecology and evolution comprable to other disciplines. QRPs include practices such as cherry-picking, p-hacking and hypothesising after results are known (HARKing), and result in low rates of reproducibility, contributing to biased accounts of the subject domain in the body of literature.</description>
    </item>
    
    <item>
      <title>Bayesianism and questionable research practices</title>
      <link>/posts/bayesianism-and-questionable-research-practices/</link>
      <pubDate>Wed, 22 Aug 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/bayesianism-and-questionable-research-practices/</guid>
      <description>What are the different model types / applications used in ecology / conservation? what is the process for deriving, evaluating, and reporting these models? At what point in the process might QRPs arise? Can the same QRP arise at different points during the analysis?  Next actions
 read the stopping rules paper I just imported into papers add prior selection / weighting and other qaeco retreat points to the table below can we generalise overarching workflows across methods?</description>
    </item>
    
    <item>
      <title>Arguments against the existance and extent of the reproducibility crisis</title>
      <link>/posts/arguments-against-the-existance-and-extent-of-the-reproducibility-crisis/</link>
      <pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/arguments-against-the-existance-and-extent-of-the-reproducibility-crisis/</guid>
      <description>Despite a growing body of large-scale meta-analyses across many different disciplines, debate as to whether there is a “crisis” persists. Fanelli et al. (2018) use a failed replication of a large-scale meta-analysis to argue that the “crisis” is mistaken, and should instead be re-branded as a narrative of “epochal changes and empowerment of science” (Jamieson 2018). In a ‘post-truth’ era of ‘alternative-facts’, how scientists communicate research on the robustness of science and its self-correcting mechanisms is certainly important (Sutherland and Wordley 2017).</description>
    </item>
    
    <item>
      <title>proposal out-takes</title>
      <link>/posts/proposal_outtakes/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/proposal_outtakes/</guid>
      <description>Unpacking what this (“reproducibility issues”) means … what do we need to know? what is my task a. how to measure the likely reproducibility of a study b. what is the function / role of different types of replications in terms of what they tell us about the broader state of the literature (validity, generalisations) c.
how widespread the reproducibility issues identified in part I. The work from the first aim will help to inform the scoping rules and coding criteria for the systematic review.</description>
    </item>
    
    <item>
      <title>Decision Science Vocabulary</title>
      <link>/posts/decision_science_vocabulary/</link>
      <pubDate>Wed, 16 May 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/decision_science_vocabulary/</guid>
      <description>Decision Making: &amp;quot; p.56: For the purposes of this paper, we define decision-making as the process of identifying options and selecting a feasible solution, based on evidence combined with the decisionmaker’s values and experience (DeFries &amp;amp; Nagendra, 2017). – Highlighted 21 May 2018&amp;quot; (Mukherjee et al. 2018)
When is a tool a tool or a system? See heading “decision support systems” in (Dicks, Walsh, and Sutherland 2014) Dicks, L. V., Walsh, J.</description>
    </item>
    
    <item>
      <title>reproducibility for decision support tools</title>
      <link>/posts/reproducibility-for-decision-support-tools/</link>
      <pubDate>Wed, 21 Mar 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/reproducibility-for-decision-support-tools/</guid>
      <description>I&amp;rsquo;ve set aside this document for developing a parallel framework of reproducibility issues not just outside of NHST, but particular to Decision Support Tools in ecology and conservation.
WHY? Because the majority of work addressing reproducibility in and outside of ecology has focused on hypothesis testing (NHST, predominantly). And non-NHST methods are important tools in the Decision Support toolbox for conservation science and applied ecology.
From this, I hope to: a) develop the coding criteria for the systematic review b) propose some sort of gold-standard protocol for developing reproducible decision support tools</description>
    </item>
    
    <item>
      <title>QRP and Biases Roadmap</title>
      <link>/posts/qrp_roadmap/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/qrp_roadmap/</guid>
      <description>Goal I want to generate a “roadmap” of the sources of bias and questionable research practices (QRPs) that I think are frequently encountered when developing D in applied ecology / conservation. These biases will reduce the reproducibility of of a given decision support tool. Identifying where in the DST development process particular biases are likely to occur should serve as a launching point for proposing solutions towards minimising their occurrence and therefore increasing the reproducibility of DSTs.</description>
    </item>
    
  </channel>
</rss>