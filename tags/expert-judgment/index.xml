<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Expert Judgment on Electronic Lab Notebook</title>
    <link>/tags/expert-judgment/</link>
    <description>Recent content in Expert Judgment on Electronic Lab Notebook</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>elise.gould@unimelb.edu.au (Elise Gould)</managingEditor>
    <webMaster>elise.gould@unimelb.edu.au (Elise Gould)</webMaster>
    <copyright>(c) 2018 -- All rights reserved.</copyright>
    <lastBuildDate>Sat, 18 Aug 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/expert-judgment/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Meeting 16 August 2018</title>
      <link>/posts/meeting-16-august-2018/</link>
      <pubDate>Sat, 18 Aug 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/meeting-16-august-2018/</guid>
      <description>Survey Sampling Design FF queried elicitation of non-experts about the prevalence of QRPs across methods they were familiar with, but hadn’t implemented themselves. Encouraged solid justification for doing this. The only reason that comes to mind for her is if increasing the sample size is the aim. But otherwise, QRP research seems to show that people are particularly forthcoming when it comes to self-reporting. So any arguments about issues around self-reporting affecting the prevalence estimates are basically moot.</description>
    </item>
    
    <item>
      <title>Morgan et al 2017 Use (and abuse) of expert elicitation in support of decision making for public policy</title>
      <link>/posts/morgan-et-al-2017-use-and-abuse-of-expert-elicitation-in-support-of-decision-making-for-public-policy/</link>
      <pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/morgan-et-al-2017-use-and-abuse-of-expert-elicitation-in-support-of-decision-making-for-public-policy/</guid>
      <description>Morgan (2014) present a review of expert elicitation methods for decision support in public policy. I reviewed this paper looking for information on how to design my methods for the expert eliciation of judgments about the consequences of particular research practices on the risk of a type I error in models used in decision support in ecology and conservation.
 I draw on relevant literature and 35 y of personal experience in designing and conducting substantively detailed expert elicitations, to suggest when it does and does not make sense to perform elicitations, how they should be designed and conducted, and how I believe the results should and should not be used.</description>
    </item>
    
    <item>
      <title>French 2012 meta-analysis for expert judgment</title>
      <link>/posts/french-2012-meta-analysis-for-expert-judgment/</link>
      <pubDate>Tue, 06 Mar 2018 00:00:00 +0000</pubDate>
      <author>elise.gould@unimelb.edu.au (Elise Gould)</author>
      <guid>/posts/french-2012-meta-analysis-for-expert-judgment/</guid>
      <description>French, S. (2012) Expert Judgment, Meta-analysis, and Participatory Risk Analysis. Decision Analysis. 9, 119–127.
Problem / background and paper goals Page 2 [@French:2012di] describes three types of expert elicitation problems. The third forms the focus of the paper:
 The expert problem: where the decision-maker does not have the domain knowledge and elicits judgments from a group of experts. The group decision problem: where the expert group itself is jointly responseible for the decision.</description>
    </item>
    
  </channel>
</rss>